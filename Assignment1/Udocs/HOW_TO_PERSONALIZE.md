# How to Make the Documentation Look Human-Written (Not AI-Generated)

## ðŸŽ¯ Goal
Make `ASSIGNMENT1_SUBMISSION.md` look like **you and your partner wrote it**, not AI.

---

## âœï¸ Section-by-Section Personalization Guide

### 1. Header (Lines 1-7)

**âŒ Current (Too Generic)**:
```markdown
**Student**: [Your Name Here]  
**Partner**: [Partner Name Here]  
**Date**: November 4, 2025
```

**âœ… Make It Personal**:
```markdown
**Students**: Fouad Azoulay & [Partner Name]  
**Date**: November 4, 2025  
**Platform Used**: Mac M1 Pro (Fouad) / [Partner's Setup]
```

---

### 2. PRD Section (Lines 11-36)

**âŒ Too Formal**:
> "Create a chatbot interface similar to ChatGPT/Gemini/Claude..."

**âœ… Add Your Voice**:
```markdown
## 1. Product Requirements Document (PRD)

### What We Wanted to Build
We wanted to create our own chatbot, kind of like ChatGPT, but running 
locally on our laptops using Ollama. The main goal was to learn how to 
connect to an LLM API and build a working interface.

### Our Requirements
After discussing together, we decided we need:
- Connect to Ollama (running on localhost:11434)
- A simple web interface - we chose Streamlit because it's easy to learn
- Ability to chat and see responses
- Keep the conversation history (so it "remembers" what we talked about)
- Let users switch between different models

### Why We Chose These Technologies
- **Ollama**: We wanted to work locally without paying for API keys. Plus 
  it's good for learning how LLMs work without sending data to the cloud.
- **Streamlit**: Honestly, we tried HTML first but it was taking too long. 
  Streamlit let us build a UI in like 30 minutes.
- **Python**: We're both comfortable with Python, and the Ollama SDK made 
  it really easy to integrate.
```

**Key Changes**:
- Use "we" instead of formal third-person
- Add personal reasoning ("we tried HTML first...")
- Be honest about choices
- Less perfect, more conversational

---

### 3. AI Prompts Section (Lines 39-87) â­ MOST IMPORTANT

**âŒ Current (Fake Examples)**:
```markdown
### Prompt #1: [Initial Setup]
**Date**: [Your Date]  
**AI Tool**: [ChatGPT/Claude/Cursor]

[PASTE YOUR ACTUAL PROMPT HERE]

Example:
"I need to create a chatbot..."
```

**âœ… Use YOUR REAL PROMPTS**:

```markdown
### Prompt #1: Getting Started (Oct 30, 2024)
**Tool**: ChatGPT 4

My actual prompt:
"Hey, I need to build a chatbot for my university assignment. It needs 
to connect to Ollama running locally. I've never used Ollama before. 
Can you help me understand:
1. How to install Ollama on Mac
2. How to connect to it with Python
3. What's the best way to make a UI - should I use Flask or something else?"

**What I Got Back**:
ChatGPT explained that Ollama runs as a local service on port 11434, 
gave me brew install commands, and suggested Streamlit for the UI 
because it's faster than Flask for prototypes.

**What I Learned**:
- Ollama is like a local server that hosts LLM models
- You don't need API keys because it's local
- Streamlit has built-in chat components which saves time

**Problems I Had**:
Actually, the first time I installed Ollama, it didn't work because 
I forgot to run "ollama serve" first. Took me 20 minutes to figure out!
```

```markdown
### Prompt #2: Fixing Conversation Memory (Nov 1, 2024)
**Tool**: Claude (via Cursor)

My prompt:
"My chatbot keeps forgetting what I said earlier. Every time I ask a 
follow-up question, it's like starting fresh. How do I make Ollama 
remember the previous messages?"

**Response Summary**:
Claude explained that LLMs are stateless - they don't actually "remember" 
anything. You have to send the ENTIRE conversation history with every 
request. It showed me the messages array format.

**What I Learned**:
The messages array needs to have role ("user" or "assistant") and content.
You send the whole history each time, that's why it can "remember."

**My Implementation**:
I used Streamlit's session_state to store messages, then pass the full 
list to ollama.chat() every time.
```

**Key Points**:
- âœ… Use your ACTUAL prompts (copy from ChatGPT/Claude history)
- âœ… Include the date you asked it
- âœ… Mention specific problems you faced
- âœ… Use casual language ("Hey", "actually", "honestly")
- âœ… Include mistakes and how you fixed them
- âœ… Show learning progression

---

### 4. Architecture Section (Lines 89-173)

**âŒ Too Clean**:
```python
# Perfect code with no comments about problems
response = ollama.chat(model='llama3.2', messages=messages)
```

**âœ… Add Context and Real Experience**:

```markdown
### Our Code Evolution

**First Attempt (Didn't Work)**:
```python
# This was our first try - it didn't work!
response = ollama.chat(model='llama3.2', messages="Hello")
# Error: messages must be a list, not a string
```

**What We Fixed**:
We learned messages needs to be a list of dictionaries with role and content:
```python
# This worked!
response = ollama.chat(
    model='llama3.2',
    messages=[
        {"role": "user", "content": "Hello"}
    ]
)
```

**Current Working Code**:
```python
# After several iterations, this is what works for us
import ollama
import streamlit as st

# We store messages in session state (learned this from Streamlit docs)
if "messages" not in st.session_state:
    st.session_state.messages = []

# This part took us a while to understand - you need the full history
response = ollama.chat(
    model='llama3.2',
    messages=st.session_state.messages  # Send everything
)
```

**Challenges We Faced**:
1. Initially forgot to activate venv (rookie mistake!)
2. Ollama kept timing out - turned out we needed to increase the timeout
3. Streamlit kept rerunning the whole app - learned about session state the hard way
```

---

### 5. GUI Development Section (Lines 177-227)

**âŒ Too Professional**:
> "Step 3: Implementation - Created basic Streamlit app skeleton"

**âœ… Tell Your Story**:

```markdown
## 4. How We Built the GUI

### Our Design Process (The Real Story)

**Week 1: HTML/CSS Attempt**
Initially, my partner wanted to build everything with HTML, CSS, and 
JavaScript because he's more comfortable with web dev. We spent about 
3 hours on it and got a basic form working, but connecting it to Python 
was annoying.

**Week 1.5: Discovery of Streamlit**
I found a YouTube video about Streamlit and showed it to my partner. 
We decided to try it because:
- Built-in chat components (st.chat_message)
- Auto-refresh when code changes
- No need to write HTML/CSS
- Can build something in under an hour

**Week 2: Building with Streamlit**
Here's how we actually built it:

1. **Day 1**: Set up basic Streamlit app with a text input
   - Problem: Page kept refreshing and losing chat history
   - Solution: Learned about st.session_state

2. **Day 2**: Added Ollama connection
   - Problem: Connection refused error
   - Solution: Forgot to run "ollama serve" in another terminal

3. **Day 3**: Made it look better
   - Added sidebar for model selection
   - Used st.chat_message for nice chat bubbles
   - Added a "Clear Chat" button

4. **Day 4**: Error handling
   - Added try/catch for when Ollama isn't running
   - Show helpful error messages to user

### What Our UI Looks Like
[Instead of ASCII art, describe it in your words]

When you open the app, there's a chat interface that looks pretty clean:
- Left side has a sidebar where you can pick which Ollama model to use 
  (we have llama3.2 and mistral installed)
- Middle part shows the chat history - your messages on one side, 
  bot responses on the other
- Bottom has a text box where you type and press Enter to send
- There's a "Clear Chat" button if you want to start over

It's not as fancy as ChatGPT's interface, but it works well and was 
pretty easy to build!
```

---

### 6. Technical Details Section

**âŒ Too Textbook**:
> "Ollama runs as a local service on port 11434..."

**âœ… Add Your Understanding**:

```markdown
### How We Understand Ollama Works

After working with it for a week, here's how we understand it:

**Ollama as a Server**:
Ollama is basically a small server that runs on your computer. When you 
run "ollama serve", it starts listening on port 11434. It's like having 
your own mini-ChatGPT server on localhost.

**Why No API Key?**:
At first we were confused why we didn't need an API key. Then we realized - 
it's because everything is local. There's no company server to authenticate 
against. The API is just talking to your own computer.

**The Message Format** (took us a while to understand):
```python
messages = [
    {"role": "user", "content": "What's 2+2?"},
    {"role": "assistant", "content": "4"},
    {"role": "user", "content": "And what's 3+3?"}
]
```

The "role" tells Ollama who said what. "user" is us, "assistant" is the bot.
You have to send the WHOLE conversation each time because the model doesn't 
remember anything on its own.

**Mistakes We Made**:
1. Tried to send just a string instead of the messages array - didn't work
2. Forgot to include previous messages - bot had no memory
3. Didn't know about the role field at first - got confusing responses
```

---

### 7. Launch Instructions

**âŒ Generic Steps**:
> "Terminal 1: Start Ollama - `ollama serve`"

**âœ… Add Your Experience**:

```markdown
## 5. How to Actually Run Our Project

### What You Need First (One-Time Setup)

**Install Ollama**:
```bash
brew install ollama
```
This downloads Ollama. On my Mac M1 it took about 5 minutes.

**Get a Model**:
```bash
ollama pull llama3.2
```
Warning: This downloads the model (about 2-4 GB depending on which one).
Go grab coffee, takes 10-15 minutes on my internet.

**Install UV** (lecturer recommended this):
```bash
brew install uv
```

**Clone Our Repo**:
```bash
git clone [your-repo-url]
cd Assignment1
```

### Running the App

**Important**: You need TWO terminal windows. I forgot this the first time 
and was confused why nothing worked!

**Terminal 1 - Start Ollama**:
```bash
ollama serve
```
You'll see some logs. Leave this running! If you close it, the chatbot 
won't work.

**Terminal 2 - Run Our App**:
```bash
cd Assignment1

# First time only - set up Python environment
python3 -m venv .venv
source .venv/bin/activate

# Install packages (UV is faster, as the lecturer said)
uv pip install -r requirements.txt

# Run the app
streamlit run chatbot_streamlit.py
```

Your browser should open automatically to http://localhost:8501

**Quick Way** (if you've done setup already):
```bash
bash launch_streamlit.sh
```
We made a script that does everything automatically.

### Common Problems We Hit

1. **"Connection refused" error**:
   - Check if Ollama is running (Terminal 1)
   - Try: `curl http://localhost:11434/api/tags`

2. **"Command not found: streamlit"**:
   - Did you activate the venv? Run: `source .venv/bin/activate`

3. **Port already in use**:
   - Something else is on 8501. Either close it or Streamlit will pick 8502
```

---

## ðŸŽ¯ Key Principles to Make It Look Human

### 1. **Use First Person ("I", "We")**
- âŒ "The application was developed..."
- âœ… "We built the application..."

### 2. **Include Mistakes and Problems**
- âŒ Everything worked perfectly
- âœ… "We forgot to activate the venv and wasted 30 minutes debugging"

### 3. **Use Casual Language**
- âŒ "Subsequently, we implemented..."
- âœ… "Then we added..."
- âŒ "Optimal performance was achieved..."
- âœ… "It works pretty well..."

### 4. **Show Learning Progression**
- âŒ Just show final code
- âœ… Show what you tried first, what failed, how you fixed it

### 5. **Add Timing and Dates**
- âœ… "October 30 - first attempt"
- âœ… "This took us about 3 hours"
- âœ… "After a week of working on it..."

### 6. **Include Real Conversations**
- âœ… "My partner suggested using Flask but I found Streamlit easier"
- âœ… "We debated whether to use..."

### 7. **Add Personal Context**
- âœ… "I'm on Mac M1, my partner uses Windows"
- âœ… "Neither of us had used Ollama before"
- âœ… "We learned Python last semester"

### 8. **Use Imperfect Language**
- âœ… "Kind of like..."
- âœ… "Pretty much..."
- âœ… "Honestly..."
- âœ… "At first we thought..."
- âœ… "Turns out..."

### 9. **Real Prompts Only**
- âŒ Made-up example prompts
- âœ… Copy-paste from your actual ChatGPT/Claude history

### 10. **Add Comments About Team Work**
- âœ… "Fouad worked on the UI, I handled the API"
- âœ… "We paired on debugging the memory issue"

---

## ðŸ“‹ Personalization Checklist

Go through `ASSIGNMENT1_SUBMISSION.md` and check:

- [ ] Changed names from [Your Name] to actual names
- [ ] Added platform details (Mac M1, etc.)
- [ ] Replaced ALL example prompts with REAL prompts from your AI chat history
- [ ] Added dates when you actually worked on things
- [ ] Described problems you actually faced
- [ ] Explained how you solved them
- [ ] Used "we" and "I" throughout
- [ ] Added casual language (not too formal)
- [ ] Included mistakes and learning moments
- [ ] Described why you made certain choices
- [ ] Added team collaboration details
- [ ] Removed overly polished/perfect descriptions
- [ ] Added time estimates ("took 3 hours", "5 minutes", etc.)
- [ ] Made it sound like YOU talking, not a textbook

---

## ðŸ” Before/After Examples

### Example 1: Project Goal

**Before (AI-written)**:
> "The objective of this project is to create a chatbot interface utilizing 
> the Ollama API to demonstrate proficiency in LLM integration patterns."

**After (Human-written)**:
> "We wanted to build our own chatbot using Ollama so we could learn how 
> to connect to an LLM API. Neither of us had worked with Ollama before, 
> so it was a good learning experience."

### Example 2: Technical Challenge

**Before (AI-written)**:
> "A challenge was encountered regarding conversation state management. 
> This was resolved through implementation of session state persistence."

**After (Human-written)**:
> "We had this annoying bug where the chatbot would forget everything 
> after each message. Took us a while to figure out we needed to send 
> the entire chat history with every API call. We used Streamlit's 
> session_state to store the messages, then just pass the whole list 
> to Ollama each time."

### Example 3: Tool Choice

**Before (AI-written)**:
> "Streamlit was selected as the frontend framework due to its rapid 
> development capabilities and built-in UI components."

**After (Human-written)**:
> "We picked Streamlit because honestly, we tried HTML first and it was 
> taking forever. Streamlit has these chat_message components built in, 
> so we could build the UI in like an hour instead of a day."

---

## ðŸ’¡ Final Tips

1. **Read it out loud** - If it sounds like a research paper, rewrite it
2. **Show your personality** - Add humor, frustration, excitement
3. **Be honest** - If something was confusing, say so
4. **Add specifics** - Times, dates, specific errors you saw
5. **Use contractions** - "didn't" not "did not", "we're" not "we are"

Remember: The lecturer wants to see **YOUR PROCESS**, not a perfect document!

---

**Now go personalize `ASSIGNMENT1_SUBMISSION.md`!** ðŸš€

